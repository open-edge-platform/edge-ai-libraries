apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.ovms.script.name }}
  labels:
    app: {{ .Values.ovms.script.name }}

data:
  init-script.sh: |-
    #!/bin/bash
    export PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"
    pip3 install optimum-intel@git+https://github.com/huggingface/optimum-intel.git  openvino-tokenizers[transformers]==2024.4.* openvino==2024.4.* nncf==2.14.0 "transformers<4.45"

    # Log in to Hugging Face using the provided token
    huggingface-cli login --token $2

    # Check if the login was successful
    if [ $? -eq 0 ]; then
        echo "Successfully logged in to Hugging Face!"
    else
        echo "Failed to log in to Hugging Face. Please check your token and try again."
    fi

    model=$0
    weight_format=$1
    model_directory_name="converted_model"

    # Convert tokenizer and export model to OpenVINO IR format
    convert_tokenizer -o "${model_directory_name}" --utf8_replace_mode replace --with-detokenizer --skip-special-tokens --streaming-detokenizer --not-add-special-tokens "${model}"

    optimum-cli export openvino --disable-convert-tokenizer --model "${model}" --weight-format "${weight_format}" "${model_directory_name}"

    # Create graph.pbtxt file
    cat <<EOF > "${model_directory_name}/graph.pbtxt"
    input_stream: "HTTP_REQUEST_PAYLOAD:input"
    output_stream: "HTTP_RESPONSE_PAYLOAD:output"
    node: {
        name: "LLMExecutor"
        calculator: "HttpLLMCalculator"
        input_stream: "LOOPBACK:loopback"
        input_stream: "HTTP_REQUEST_PAYLOAD:input"
        input_side_packet: "LLM_NODE_RESOURCES:llm"
        output_stream: "LOOPBACK:loopback"
        output_stream: "HTTP_RESPONSE_PAYLOAD:output"
        input_stream_info: {
            tag_index: "LOOPBACK:0",
            back_edge: true
        }
        node_options: {
            [type.googleapis.com / mediapipe.LLMCalculatorOptions]: {
                models_path: "./",
                plugin_config: '{"KV_CACHE_PRECISION": "u8", "DYNAMIC_QUANTIZATION_GROUP_SIZE": "32"}',
                cache_size: 4
            }
        }
        input_stream_handler {
            input_stream_handler: "SyncSetInputStreamHandler",
            options {
                [mediapipe.SyncSetInputStreamHandlerOptions.ext] {
                    sync_set {
                        tag_index: "LOOPBACK:0"
                    }
                }
            }
        }
    }
    EOF
    
    cp -r converted_model /app/data

    echo "All the steps are completed successfully"

  config-converted_model.json: |-
    {
        "model_config_list": [],
        "mediapipe_config_list": [
            {
                "name": "{{ .Values.global.LLM_MODEL }}",
                "base_path": "/app/data/converted_model"
            }
        ]
    }