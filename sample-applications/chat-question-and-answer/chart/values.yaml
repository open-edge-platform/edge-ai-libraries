global:
  huggingface:
    apiToken:  # Set this during installation
  proxy:
    no_proxy:
    http_proxy:
    https_proxy:
  UI_NODEPORT: 
  POSTGRES_USER:
  POSTGRES_PASSWORD:
  MINIO_ROOT_USER:
  MINIO_ROOT_PASSWORD:
  LLM_MODEL:
  EMBEDDING_MODEL_NAME:
  RERANKER_MODEL:
  OTLP_ENDPOINT:
  OTLP_ENDPOINT_TRACE:
  #Set either one of the below embedding model sever to true by defauly OVMS  is enabled
  teiEmbeddingService:
    enabled: false
  ovmsEmbeddingService:
    enabled: true
  ovmsEmbeddingGPUService:
    enabled: false 
  GPU: 
    enabled: true 
    key:
    value:
  egai_minio_pvc:
    size: 20Gi
  egai_reranker_pvc:
    size: 20Gi
  egai_ovms_pvc:
    size: 20Gi
  egai_ovms_embed_pvc:
    size: 20Gi
  pvc_policy: # pvc_policy: keep ; to persist models across multiple deployments

Chatqna:
  name: chatqna-backend
  image:
    repository: intel/chatqna
    tag: "1.1.2"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8100
    targetPort: 8080
  env:
    ENDPOINT_URL: http://ovms
    INDEX_NAME: intel-rag
    FETCH_K: 10
    PORT_DB: 5432/langchain
    SERVICE_NAME: chatqna
    SERVICE_ENV: chatqna
    OTEL_METRICS_EXPORTER: otlp
    OTEL_TRACES_EXPORTER: otlp
    OTEL_EXPORTER_OTLP_TRACES_PROTOCOL: http/protobuf
    REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
    PG_CONNECTION_STRING: postgresql+psycopg://

dataprepPgvector:
  name: document-ingestion
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

tgiService:
  name: text-generation-service
  enabled: false
  service:
    port: 8080
vllmService:
  name: vllm-service
  enabled: false
  service:
    port: 8080
ovmsService:
  name: ovmsService
  enabled: true
  service:
    port: 8300
reranker:
  service:
    port: 8090
chatqnaui:
  name: chatqna-ui
  nameOverride: "ui"
  service:
    port: 80
