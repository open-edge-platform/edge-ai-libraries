# Default configuration values for the document summarization Helm chart
global:
  proxy:
    noProxy: "<no-proxy>"
    httpProxy: "<http-proxy>"
    httpsProxy: "<https-proxy>"
  huggingface:
    token: "<huggingface-token>"
  ui:
    nodePort: <ui-node-port>
  otlp:
    otlpEndPoint: "<otlp end point>"
  llm:
    llmModelId: "Intel/neural-chat-7b-v3-3"
    llmEndpointUrl: "http://ovms-service:8300"  
  backendApi:
    url: "http://docsum-api:8090/v1/docsum"
  ovms:
    weightFormat: "int8"
    targetDevice: "CPU"
  ovms_pvc:
    size: 20Gi
  ovms_embed_pvc:
    size: 20Gi
  keeppvc: false # true  to persist models across multiple deployments

  DocSum:
    name: document-summarization
    image:
      ui:
        repository: "amr-fm-registry.caas.intel.com/esh-user/genai-docsum"
        tag: "2.0"
        pullPolicy: IfNotPresent
      backend:
        repository: "amr-fm-registry.caas.intel.com/esh-user/genai-docsum"
        tag: "2.0"
        pullPolicy: IfNotPresent
    
nginx:
  image:
    repository: nginx
    tag: 1.27.1
    pullPolicy: IfNotPresent
    

docSumUI:
  name: docsum-ui  
  service:
    type: ClusterIP
    port: 9998
    targetPort: 7860
  env:
    gradioPort: 7860

# Resource requests and limits
resources:
  limits:
    cpu: "1"
    memory: "1Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"